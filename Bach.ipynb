{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3032cf59-a90c-473a-aaea-7a52d7006e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pretty_midi\n",
    "import tensorflow as tf\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30bbe2a7-73d9-487b-8fea-59fdcdab33b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Sampling rate for audio playback\n",
    "# _SAMPLING_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "242e2f98-1902-4a97-941b-659a9987e937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tonyfernandes/Desktop/Projects/ML/BachHarmonization/Jsb16thSeparated.json\n"
     ]
    }
   ],
   "source": [
    "dir_path = os.path.abspath(\"./Jsb16thSeparated.json\")\n",
    "print(dir_path)\n",
    "\n",
    "file = open(dir_path)\n",
    "\n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac277724-2eb5-4bc2-8e1b-c09cca3dd819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55228, 4)\n",
      "(18408, 4)\n",
      "(18900, 4)\n",
      "[-1, 81]\n",
      "[0, 82]\n"
     ]
    }
   ],
   "source": [
    "rawTEST = data[\"test\"]\n",
    "rawTRAIN = data[\"train\"]\n",
    "rawCV = data[\"valid\"]\n",
    "def data_to_numpy(list):\n",
    "    gigalist = np.array(list[0])\n",
    "    for i in range(1, len(list)):\n",
    "        gigalist = np.append(gigalist, np.array(list[i]), axis=0)\n",
    "    return gigalist\n",
    "\n",
    "TRAIN = data_to_numpy(rawTRAIN)\n",
    "TEST = data_to_numpy(rawTEST)\n",
    "CV = data_to_numpy(rawCV)\n",
    "\n",
    "print(TRAIN.shape)\n",
    "print(CV.shape)\n",
    "print(TEST.shape)\n",
    "\n",
    "\n",
    "def min_max_find(TRAIN):\n",
    "    min = TRAIN[0,0]\n",
    "    max = TRAIN[0,0]\n",
    "    for time in range(TRAIN.shape[0]):\n",
    "        for voicing in range(4):\n",
    "            if(TRAIN[time, voicing] < min):\n",
    "                min = TRAIN[time, voicing]\n",
    "            if(TRAIN[time, voicing] > max):\n",
    "                max = TRAIN[time, voicing]\n",
    "    return [min, max]\n",
    "\n",
    "print(min_max_find(TRAIN))\n",
    "print(min_max_find(np.add(TRAIN, 1)))\n",
    "#min_max_find(TEST)\n",
    "#min_max_find(CV)\n",
    "\n",
    "TRAIN = np.add(TRAIN, 1)\n",
    "TEST = np.add(TEST, 1)\n",
    "CV = np.add(CV, 1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf5d8142-20d6-4079-a24a-f52803b077ff",
   "metadata": {},
   "source": [
    "Note that the original input had MIDI notes in the range [0,81], as well as -1 representing no note. So the total range of integers was [-1, 81]. I shifted it by 1 to be [0,82]. Now 0 represents no note. The pitches will have to be shifted back by -1 after the neural network is done with them."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a09b608-1a0b-4e2c-8b9e-f0f2ba2fbbff",
   "metadata": {},
   "source": [
    "According to how I used mdoel.train_on_batch(inputs, [alto_output, tenor_output, bass_output]), the inputs shape should be (batch_size, sequence_length), and the targets (3, batch_size, sequence_length). Now, just how do we do this? We need to split each individual voicing into sequences. Then, we need to batch each individual voicing. Then, at the end, we need to define inputs to be the soprano voice only and the targets to be [alto, tenor, bass] voices.\n",
    "\n",
    "We may not be able to rely on this same pipeline. Do we need the from_tensor_slices? Ultimately, we simply need to split each voicing into sequences (drop the remainder), then split them into batches (and drop the remainder), shuffle, cache, and \"prefetch\". Only after that do we define out dataset to be dataset = soprano_voicing, [alto_voicing,tenor_voicing,bass_voicing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "078e244c-3fad-4d6d-a60d-c017239e998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 64\n",
    "\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(TRAIN)\n",
    "sequences = ids_dataset.batch(seq_length, drop_remainder=True)\n",
    "\n",
    "def split_input_target(sequence):\n",
    "    input = sequence[:,0]\n",
    "    target = sequence[:, 1:]\n",
    "    \n",
    "    return input, target\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3933f098-ef8e-4a84-8770-047d13870a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch the dataset for input\n",
    "batch_size = 60\n",
    "buffer_size = TRAIN.shape[0]//10  # the number of items in the dataset\n",
    "batched_ds = (dataset\n",
    "            .shuffle(buffer_size)\n",
    "            .batch(batch_size, drop_remainder=True)\n",
    "            .cache()\n",
    "            .prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bde19212-f820-4621-9114-3ea27c442abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 64, 32)       2656        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 64, 128)      82432       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 64, 83)      10707       ['lstm[0][0]']                   \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, 64, 83)      10707       ['lstm[0][0]']                   \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " time_distributed_2 (TimeDistri  (None, 64, 83)      10707       ['lstm[0][0]']                   \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack (TFOpLambda)          (None, 64, 3, 83)    0           ['time_distributed[0][0]',       \n",
      "                                                                  'time_distributed_1[0][0]',     \n",
      "                                                                  'time_distributed_2[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 117,209\n",
      "Trainable params: 117,209\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 13:01:59.528181: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-08-18 13:01:59.528873: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-08-18 13:01:59.529228: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense, Input, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "input_shape = (seq_length,)\n",
    "num_notes = 83\n",
    "embed_dim = 32\n",
    "lstm_lyrs = 128\n",
    "\n",
    "input_ = Input(shape=input_shape)\n",
    "x = Embedding(num_notes, embed_dim, input_length=seq_length)(input_)\n",
    "x2 = LSTM(lstm_lyrs, return_sequences=True, activation='tanh')(x)\n",
    "\n",
    "outputs = []\n",
    "for _ in range(3):  # Alto, Tenor, Bass\n",
    "    output = TimeDistributed(Dense(num_notes, activation='softmax'))(x2)\n",
    "    outputs.append(output)\n",
    "\n",
    "# Stack the outputs into a single tensor\n",
    "output_ = tf.stack(outputs, axis=2)\n",
    "\n",
    "model = Model(inputs=input_, outputs=output_)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # y_true shape: (batch_size, seq_length, 3)\n",
    "    # y_pred shape: (batch_size, seq_length, 3, num_notes)\n",
    "    return SparseCategoricalCrossentropy()(y_true, y_pred)\n",
    "\n",
    "# In the create_model function:\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=custom_loss\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a85026c-4e14-46fb-8fa4-2033faac0ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 13:02:03.034663: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int64 and shape [55228,4]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-08-18 13:02:03.035027: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int64 and shape [55228,4]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-08-18 13:02:03.135590: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-08-18 13:02:03.136174: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-08-18 13:02:03.136612: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-08-18 13:02:03.313612: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-08-18 13:02:03.314141: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-08-18 13:02:03.314525: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-08-18 13:02:03.412398: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 38ms/step - loss: 4.2992\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 1s 38ms/step - loss: 3.1459\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 1s 39ms/step - loss: 2.7521\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 1s 39ms/step - loss: 2.6922\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 1s 39ms/step - loss: 2.6711\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 2.6597\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 2.6507\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 2.6441\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 2.6380\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 2.6326\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 2.6274\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 1s 41ms/step - loss: 2.6221\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 2.6161\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 2.6087\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 2.5989\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 2.5849\n",
      "Epoch 17/100\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 2.5605\n",
      "Epoch 18/100\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 2.5324\n",
      "Epoch 19/100\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 2.5026\n",
      "Epoch 20/100\n",
      "14/14 [==============================] - 1s 41ms/step - loss: 2.4761\n",
      "Epoch 21/100\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 2.4519\n",
      "Epoch 22/100\n",
      "14/14 [==============================] - 1s 41ms/step - loss: 2.4284\n",
      "Epoch 23/100\n",
      "14/14 [==============================] - 1s 41ms/step - loss: 2.4065\n",
      "Epoch 24/100\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 2.3866\n",
      "Epoch 25/100\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 2.3678\n",
      "Epoch 26/100\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 2.3492\n",
      "Epoch 27/100\n",
      "14/14 [==============================] - 1s 41ms/step - loss: 2.3307\n",
      "Epoch 28/100\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 2.3129\n",
      "Epoch 29/100\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 2.2961\n",
      "Epoch 30/100\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 2.2802\n",
      "Epoch 31/100\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 2.2656\n",
      "Epoch 32/100\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 2.2526\n",
      "Epoch 33/100\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 2.2411\n",
      "Epoch 34/100\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 2.2309\n",
      "Epoch 35/100\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 2.2216\n",
      "Epoch 36/100\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 2.2131\n",
      "Epoch 37/100\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 2.2051\n",
      "Epoch 38/100\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 2.1975\n",
      "Epoch 39/100\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 2.1903\n",
      "Epoch 40/100\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 2.1834\n",
      "Epoch 41/100\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 2.1767\n",
      "Epoch 42/100\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 2.1701\n",
      "Epoch 43/100\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 2.1636\n",
      "Epoch 44/100\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 2.1571\n",
      "Epoch 45/100\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 2.1503\n",
      "Epoch 46/100\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 2.1430\n",
      "Epoch 47/100\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 2.1348\n",
      "Epoch 48/100\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 2.1248\n",
      "Epoch 49/100\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 2.1142\n",
      "Epoch 50/100\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 2.1030\n",
      "Epoch 51/100\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 2.0929\n",
      "Epoch 52/100\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 2.0833\n",
      "Epoch 53/100\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 2.0756\n",
      "Epoch 54/100\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 2.0693\n",
      "Epoch 55/100\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 2.0640\n",
      "Epoch 56/100\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 2.0595\n",
      "Epoch 57/100\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 2.0554\n",
      "Epoch 58/100\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 2.0515\n",
      "Epoch 59/100\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 2.0477\n",
      "Epoch 60/100\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 2.0439\n",
      "Epoch 61/100\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 2.0401\n",
      "Epoch 62/100\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 2.0366\n",
      "Epoch 63/100\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 2.0334\n",
      "Epoch 64/100\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 2.0303\n",
      "Epoch 65/100\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 2.0276\n",
      "Epoch 66/100\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 2.0250\n",
      "Epoch 67/100\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 2.0229\n",
      "Epoch 68/100\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 2.0215\n",
      "Epoch 69/100\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 2.0195\n",
      "Epoch 70/100\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 2.0170\n",
      "Epoch 71/100\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 2.0138\n",
      "Epoch 72/100\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 2.0104\n",
      "Epoch 73/100\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 2.0077\n",
      "Epoch 74/100\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 2.0055\n",
      "Epoch 75/100\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 2.0033\n",
      "Epoch 76/100\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 2.0012\n",
      "Epoch 77/100\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 1.9991\n",
      "Epoch 78/100\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 1.9970\n",
      "Epoch 79/100\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 1.9947\n",
      "Epoch 80/100\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 1.9924\n",
      "Epoch 81/100\n",
      "14/14 [==============================] - 1s 48ms/step - loss: 1.9902\n",
      "Epoch 82/100\n",
      "14/14 [==============================] - 1s 48ms/step - loss: 1.9880\n",
      "Epoch 83/100\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 1.9860\n",
      "Epoch 84/100\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 1.9839\n",
      "Epoch 85/100\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 1.9820\n",
      "Epoch 86/100\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 1.9801\n",
      "Epoch 87/100\n",
      "14/14 [==============================] - 1s 48ms/step - loss: 1.9783\n",
      "Epoch 88/100\n",
      "14/14 [==============================] - 1s 49ms/step - loss: 1.9765\n",
      "Epoch 89/100\n",
      "14/14 [==============================] - 1s 51ms/step - loss: 1.9746\n",
      "Epoch 90/100\n",
      "14/14 [==============================] - 1s 50ms/step - loss: 1.9726\n",
      "Epoch 91/100\n",
      "14/14 [==============================] - 1s 51ms/step - loss: 1.9708\n",
      "Epoch 92/100\n",
      "14/14 [==============================] - 1s 51ms/step - loss: 1.9691\n",
      "Epoch 93/100\n",
      "14/14 [==============================] - 1s 49ms/step - loss: 1.9675\n",
      "Epoch 94/100\n",
      "14/14 [==============================] - 1s 51ms/step - loss: 1.9658\n",
      "Epoch 95/100\n",
      "14/14 [==============================] - 1s 51ms/step - loss: 1.9641\n",
      "Epoch 96/100\n",
      "14/14 [==============================] - 1s 54ms/step - loss: 1.9624\n",
      "Epoch 97/100\n",
      "14/14 [==============================] - 1s 55ms/step - loss: 1.9606\n",
      "Epoch 98/100\n",
      "14/14 [==============================] - 1s 55ms/step - loss: 1.9589\n",
      "Epoch 99/100\n",
      "14/14 [==============================] - 1s 54ms/step - loss: 1.9573\n",
      "Epoch 100/100\n",
      "14/14 [==============================] - 1s 55ms/step - loss: 1.9556\n"
     ]
    }
   ],
   "source": [
    " history = model.fit(batched_ds, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a89ae28f-7d8f-4a5c-85c6-16384ab08bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_sequencing(raw_data):\n",
    "    num_sequences = (raw_data.shape[0] - seq_length) // seq_length\n",
    "    \n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(num_sequences):\n",
    "        start = i * seq_length\n",
    "        end = start + seq_length\n",
    "        \n",
    "        inputs.append(raw_data[start:end, 0])  # Soprano voice\n",
    "        targets.append(raw_data[start:end, 1:])  # Alto, Tenor, Bass voices\n",
    "        \n",
    "    return np.array(inputs), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de09dd27-b1c6-4180-8ae6-08ef0c41aea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 13:03:13.865149: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-08-18 13:03:13.865614: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-08-18 13:03:13.866086: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-08-18 13:03:14.017665: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [286,64,3]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV loss: 2.029677629470825\n",
      "Percentage of correct notes for alto,tenor,bass in CV: [0.3462631118881119, 0.3541848776223776, 0.28114073426573427]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 13:03:14.081978: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-08-18 13:03:14.082380: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-08-18 13:03:14.082776: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "CV_in, CV_target = manual_sequencing(CV)\n",
    "CV_logits = model.predict(CV_in)\n",
    "\n",
    "CV_pred = np.argmax(CV_logits, axis=-1)\n",
    "\n",
    "CV_dataset = tf.data.Dataset.from_tensor_slices((CV_in, CV_target))\n",
    "CV_dataset = CV_dataset.batch(batch_size)  \n",
    "\n",
    "loss = model.evaluate(CV_dataset, verbose=0)\n",
    "print(f'CV loss: {loss}')\n",
    "\n",
    "# Calculate accuracy for each voice\n",
    "accuracies = []\n",
    "for voice in range(3):  # Alto, Tenor, Bass\n",
    "    correct = np.sum(CV_pred[:, :, voice] == CV_target[:, :, voice])\n",
    "    total = np.prod(CV_target[:, :, voice].shape)\n",
    "    accuracy = correct / total\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "print(f'Percentage of correct notes for alto,tenor,bass in CV: {accuracies}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0018511-e243-4666-b471-721ee38aa6cc",
   "metadata": {},
   "source": [
    "We want to convert this data into a list of notes/no notes and their durations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdcb3767-0462-4166-9a8e-b86f487ba774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our data being divided by time sequences works well for processing and neural networks but \n",
    "#to play these actual notes back we need to convert each repeated number in each voicing into \n",
    "#a note with (note pitch, note duration)\n",
    "#The output of this function should be a list of the 4 voicings\n",
    "#Each voicing is a list of sequences\n",
    "#Each sequence is a list of notes\n",
    "#Each note is a tuple: (note pitch, note duration)\n",
    "#So the shape of the output is [4, num_sequences, num_notes (could be different for each sequence)] and the data type of each\n",
    "#note is a tuple of size 2\n",
    "def playback(input_soprano, ATB_predictions):\n",
    "    \n",
    "    #convert each sequence of 64 16th notes to the actual note/silence played for a duration\n",
    "    def to_real_notes(voicing):\n",
    "        num_sequences = voicing.shape[0]\n",
    "        res = [[] for _ in range(num_sequences)]\n",
    "        for sequence_index in range(num_sequences):\n",
    "            \n",
    "            seqs = voicing[sequence_index]\n",
    "            curDur = 0\n",
    "            curNote = seqs[0]\n",
    "            for i in range(seqs.shape[0]):\n",
    "                if(seqs[i] == curNote):\n",
    "                    curDur += 1\n",
    "                else:\n",
    "                    #We subtract 1 from current note because we added 1 to all notes while preprocessing input\n",
    "                    this_note = (curNote-1, curDur)\n",
    "                    res[sequence_index].append(this_note)\n",
    "                    curNote = seqs[i]\n",
    "                    curDur = 1\n",
    "                #if last note in sequence, we must save it regardless\n",
    "                if(i == seqs.shape[0] - 1):\n",
    "                    this_note = (curNote-1, curDur)\n",
    "                    res[sequence_index].append(this_note)\n",
    "            # print(f'seq_index: {sequence_index}')\n",
    "            # print(seqs)\n",
    "            # print(res[sequence_index])\n",
    "            # print(\"-\"*50)\n",
    "        #Returns a list of sequences. Each sequence contains some number of notes. \n",
    "        #Each note is represented by (note pitch, note duration) where pitch is the MIDI number and duration the number of 16th notes\n",
    "        return res\n",
    "    \n",
    "    soprano_notes = to_real_notes(input_soprano)\n",
    "    alto_notes = to_real_notes(ATB_predictions[:, :, 0])\n",
    "    tenor_notes = to_real_notes(ATB_predictions[:, :, 1])\n",
    "    bass_notes = to_real_notes(ATB_predictions[:, :, 2])\n",
    "\n",
    "    return [soprano_notes, alto_notes, tenor_notes, bass_notes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a7d5cb0-d838-4637-9f3c-1c4428ef0cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_MIDI_File(NOTES, BPM, seq_indices, out_file_name):\n",
    "    midi_data = pretty_midi.PrettyMIDI()\n",
    "    piano_program = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')\n",
    "    sop_piano = pretty_midi.Instrument(program=piano_program)\n",
    "    alto_piano = pretty_midi.Instrument(program=piano_program)\n",
    "    ten_piano = pretty_midi.Instrument(program=piano_program)\n",
    "    bass_piano = pretty_midi.Instrument(program=piano_program)\n",
    "\n",
    "    pianos = [sop_piano, alto_piano, ten_piano, bass_piano]\n",
    "    \n",
    "    #SPS = sixteenth notes per second\n",
    "    SPS = (BPM/60)*4\n",
    "\n",
    "    for voice_index in range(4):\n",
    "        time = 0\n",
    "\n",
    "        for j in range(len(seq_indices)):\n",
    "            cur_notes = NOTES[voice_index][seq_indices[j]]\n",
    "        \n",
    "            for i in range(len(cur_notes)):\n",
    "                note_pitch = cur_notes[i][0]\n",
    "                note_duration = cur_notes[i][1]/SPS\n",
    "                if(note_pitch != -1):\n",
    "                    note = pretty_midi.Note(velocity=100, pitch=note_pitch, start=time, end=time+note_duration)\n",
    "                    pianos[voice_index].notes.append(note)\n",
    "                time += note_duration\n",
    "    \n",
    "    midi_data.instruments.append(sop_piano)\n",
    "    midi_data.instruments.append(alto_piano)\n",
    "    midi_data.instruments.append(ten_piano)\n",
    "    midi_data.instruments.append(bass_piano)\n",
    "    midi_data.write(out_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12f8a8c8-215c-4e2a-adf1-f5fc2671cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_PRED_NOTES = playback(CV_in, CV_pred)\n",
    "CV_ACTUAL_NOTES = playback(CV_in, CV_target)\n",
    "\n",
    "write_MIDI_File(CV_PRED_NOTES, 80, [0, 1, 2], 'CVPred012.mid')\n",
    "write_MIDI_File(CV_ACTUAL_NOTES, 80, [0, 1, 2], 'CVActual012.mid')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a74428f-cdbe-4be8-bc0e-9e67c0a6bc11",
   "metadata": {},
   "source": [
    "After listening to these files, I realized that the most probable note at teach time step tends to be the previous one. In the ML harmonization, notes are held longer than in the actual harmonization. To combat this, I will introduce temperature-based sampling in order to not always pick the most probable note, adding a bit of variability. I will subjectively judge which voicings deserve more variability (for instance the bass voicing seems to move a lot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cebbe79-d577-4b29-9ff1-ede752482e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "TEST_in, TEST_targets = manual_sequencing(TEST)\n",
    "TEST_logits = model.predict(TEST_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "452c8628-138b-43a9-8a3a-b2f55c9e219e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred             actual\n",
      "[63 58 51]       [61 58 54]\n",
      "[63 58 51]       [61 58 54]\n",
      "[63 58 51]       [61 58 54]\n",
      "[63 58 51]       [61 58 54]\n",
      "[66 58 54]       [61 56 53]\n",
      "[66 58 54]       [61 56 53]\n",
      "[66 63 47]       [61 56 53]\n",
      "[66 63 47]       [61 56 53]\n",
      "[66 61 54]       [61 54 54]\n",
      "[66 61 54]       [61 54 54]\n",
      "[66 61 49]       [61 56 53]\n",
      "[65 61 49]       [61 56 53]\n",
      "[61 58 42]       [63 58 51]\n",
      "[61 58 42]       [63 58 51]\n",
      "[61 58 42]       [65 59 51]\n"
     ]
    }
   ],
   "source": [
    "def temperature_sampling(gye, temperatures):\n",
    "    #don't modify the original array\n",
    "    data_logits = np.copy(gye)\n",
    "    for voice in range(3):\n",
    "        data_logits[:,:,voice,:] = np.log(data_logits[:,:,voice,:]) / temperatures[voice]\n",
    "    \n",
    "    data_logits -= np.max(data_logits, axis=-1, keepdims=True)\n",
    "    data_logits = np.exp(data_logits)\n",
    "    data_logits /= np.sum(data_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "    sampling = np.zeros((data_logits.shape[0], data_logits.shape[1], 3), dtype=int)\n",
    "    \n",
    "    for seq in range(data_logits.shape[0]):\n",
    "        for step in range(data_logits.shape[1]):\n",
    "            for voice in range(data_logits.shape[2]):\n",
    "                sampling[seq, step, voice] = np.random.choice(data_logits.shape[3], p=data_logits[seq, step, voice])\n",
    "    return sampling\n",
    "\n",
    "temps = [0.001,0.001,0.002]\n",
    "\n",
    "TEST_pred = temperature_sampling(TEST_logits, temps)\n",
    "print(\"pred             actual\")\n",
    "for i in range(15):\n",
    "    print(TEST_pred[0,i], \"     \", TEST_targets[0,i])\n",
    "#print(TEST_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b55a620-fd8b-41e1-b29d-ca9e1e6ffbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PRED_NOTES = playback(TEST_in, TEST_pred)\n",
    "TEST_ACTUAL_NOTES = playback(TEST_in, TEST_targets)\n",
    "\n",
    "sequences = [[0,1,2],[3,4,5,6,7]]\n",
    "BPM = 80\n",
    "\n",
    "for i in range(len(sequences)):\n",
    "    write_MIDI_File(TEST_PRED_NOTES, BPM, sequences[i], 'TESTPred' + str(i) + '.mid')\n",
    "    write_MIDI_File(TEST_ACTUAL_NOTES, BPM, sequences[i], 'TESTActual' + str(i) + '.mid')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f300e71b-366b-4710-8d70-62815865f698",
   "metadata": {},
   "source": [
    "------CONCLUSION----------\n",
    "\n",
    "I recognize many areas of improvement here. The neural network does well in terms of predicting notes, but the musical coherency of the network could be improved. The network inherently recognizes patterns between soprano voicings, and the alto, tenor, bass voicings over time, giving it harmonies that sound alright and don't clash awkwardly. However, there must be a way for the network to understand the more intricate relationships between melody and harmony in these chorales. For example, in Bach's chorales I noticed periods of more movement in the harmony followed by periods of less movement. In my RNN, the amount of movement tends to follow the amount of movement in the soprano. In the neural network, the soprano changing notes tends to be the most reliable sign to change notes in the ATB voicings, but in Bach's actual chorales all the voicings move almost independently yet synergistically.\n",
    "\n",
    "Qualitatively, when I play a ML sequence followed by the actual sequence, the ML sequence can sound Bach-like at times, but the actual sequence conveys much greater intention and direction. A greater sense of \"harmonic direction\" would be another area of improvement. \n",
    "\n",
    "Ultimately, given how complicated Bach's 4-part harmonies are, I am happy with the result even though there are many areas to improve on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
